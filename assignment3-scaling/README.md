# CS336 Assignment 3: Scaling Laws

This assignment explores empirical scaling laws in language models, investigating how model performance scales with compute, model size, and dataset size. The goal is to understand and predict language model behavior across different scales.

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Implementation Details](#implementation-details)
- [Project Structure](#project-structure)
- [Setup](#setup)
- [Usage](#usage)
- [Computational Requirements](#computational-requirements)
- [Important Notes](#important-notes)

## Overview

Scaling laws describe how language model performance (measured by loss or perplexity) changes with:

1. **Model Size** (N): Number of non-embedding parameters
2. **Dataset Size** (D): Number of training tokens
3. **Compute Budget** (C): Total FLOPs used for training

This assignment involves:
- Training models of various sizes on different dataset sizes
- Measuring final loss and training dynamics
- Fitting power-law relationships to empirical data
- Predicting optimal model configurations for given compute budgets
- Understanding compute-optimal training (Chinchilla scaling laws)

## Implementation Details

### Key Components

#### 1. Transformer Language Model (`cs336_scaling/model.py`)

A standard Transformer decoder implementation with:
- **Token & Position Embeddings**: Learned embeddings for tokens and positions
- **Multi-Head Self-Attention**: Causal attention with dropout
- **Feed-Forward Networks**: GELU-activated FFNs
- **Layer Normalization**: Pre-norm architecture
- **Text Generation**: Autoregressive sampling with temperature/top-k

**Key Methods**:
- `forward()`: Standard forward pass returning logits
- `generate()`: Autoregressive text generation
- `from_pretrained()`: Load pre-trained models
- `get_num_params()`: Count non-embedding parameters

#### 2. Scaling Law Experiments

This assignment requires running systematic experiments to collect data points:

**Experiment Design**:
- Train models with different numbers of layers/dimensions
- Vary dataset size (e.g., 1M, 10M, 100M, 1B tokens)
- Measure loss at different training steps
- Collect compute metrics (FLOPs, training time)

**Analysis**:
- Fit power laws: L(N) = aN^(-Î±), L(D) = bD^(-Î²), L(C) = cC^(-Î³)
- Determine compute-optimal allocation between N and D
- Predict performance for unseen configurations

## Project Structure

```
assignment3-scaling/
â”œâ”€â”€ cs336_scaling/
â”‚   â”œâ”€â”€ model.py               # Transformer LM implementation
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ data/                      # Training datasets (user-provided)
â”œâ”€â”€ experiments/               # Experiment scripts and results (user-created)
â”œâ”€â”€ cs336_spring2025_assignment3_scaling.pdf  # Assignment handout
â”œâ”€â”€ pyproject.toml             # Dependencies
â”œâ”€â”€ uv.lock                    # Lock file
â””â”€â”€ README.md                  # This file
```

**Note**: This assignment is primarily experimental/analytical. You'll need to create your own:
- Training scripts
- Data processing pipelines
- Experiment tracking code
- Analysis notebooks

## Setup

### 1. Install Dependencies

```bash
# Install uv if not already installed
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install dependencies
uv sync
```

### 2. Prepare Datasets

You'll need datasets of various sizes. Options include:

**Option 1: Use Public Datasets**
```bash
mkdir -p data

# TinyStories (~1GB)
wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt -O data/tinystories.txt

# OpenWebText (~40GB)
# Download from https://huggingface.co/datasets/Skylion007/openwebtext

# The Pile (825GB)
# Download from https://pile.eleuther.ai/
```

**Option 2: Create Synthetic Datasets**
```python
# Generate datasets of controlled sizes
import random
import string

def generate_synthetic_text(num_tokens, vocab_size=10000):
    """Generate random text for scaling experiments"""
    tokens = [str(random.randint(0, vocab_size-1)) for _ in range(num_tokens)]
    return ' '.join(tokens)

# Create 1M, 10M, 100M token datasets
for size in [1_000_000, 10_000_000, 100_000_000]:
    text = generate_synthetic_text(size)
    with open(f'data/synthetic_{size}.txt', 'w') as f:
        f.write(text)
```

## Usage

### 1. Define Model Configurations

Create models of different sizes by varying hyperparameters:

```python
from cs336_scaling.model import BasicsTransformerLM

# Small model (~10M params)
model_small = BasicsTransformerLM(
    vocab_size=10000,
    context_length=256,
    d_model=256,
    num_layers=4,
    num_heads=4,
    d_ff=1024
)

# Medium model (~50M params)
model_medium = BasicsTransformerLM(
    vocab_size=10000,
    context_length=512,
    d_model=512,
    num_layers=8,
    num_heads=8,
    d_ff=2048
)

# Large model (~200M params)
model_large = BasicsTransformerLM(
    vocab_size=10000,
    context_length=1024,
    d_model=768,
    num_layers=12,
    num_heads=12,
    d_ff=3072
)
```

### 2. Run Scaling Experiments

Example training script for collecting scaling data:

```python
import torch
import numpy as np
from cs336_scaling.model import BasicsTransformerLM

def train_and_measure(model, dataset, num_steps, compute_budget):
    """
    Train model and measure loss at various checkpoints.

    Returns: dict with losses, compute used, final metrics
    """
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
    losses = []

    for step in range(num_steps):
        # Training step
        batch = get_batch(dataset)  # Your data loading logic
        logits = model(batch['input_ids'])
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)),
                               batch['labels'].view(-1))
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        # Record loss periodically
        if step % 100 == 0:
            losses.append(loss.item())

    return {
        'final_loss': losses[-1],
        'loss_curve': losses,
        'num_params': model.get_num_params(),
        'num_tokens': num_steps * batch_size * context_length
    }

# Run experiments
results = []
for model_config in [small_config, medium_config, large_config]:
    for dataset_size in [1e6, 1e7, 1e8]:
        model = BasicsTransformerLM(**model_config)
        result = train_and_measure(model, dataset_size, num_steps=10000)
        results.append(result)
```

### 3. Analyze Scaling Laws

Fit power laws to your experimental data:

```python
import numpy as np
from scipy.optimize import curve_fit

# Power law function: L = a * N^(-alpha)
def power_law(x, a, alpha):
    return a * np.power(x, -alpha)

# Collect data points
param_counts = [result['num_params'] for result in results]
final_losses = [result['final_loss'] for result in results]

# Fit power law
params, _ = curve_fit(power_law, param_counts, final_losses)
a, alpha = params

print(f"Scaling law: L(N) = {a:.4f} * N^(-{alpha:.4f})")

# Predict loss for unseen model size
new_model_size = 500_000_000  # 500M params
predicted_loss = power_law(new_model_size, a, alpha)
print(f"Predicted loss for 500M model: {predicted_loss:.4f}")
```

### 4. Compute-Optimal Training

Determine optimal model size for a given compute budget:

```python
def compute_optimal_allocation(compute_budget, alpha_N, alpha_D):
    """
    Given compute budget C and scaling exponents,
    find optimal N (params) and D (tokens).

    Chinchilla finding: N and D should scale proportionally with C.
    """
    # Chinchilla: N_opt âˆ C^0.5, D_opt âˆ C^0.5
    N_opt = (compute_budget / constant_factor) ** 0.5
    D_opt = (compute_budget / constant_factor) ** 0.5

    return N_opt, D_opt
```

## Computational Requirements

### Hardware Requirements

| Experiment Scale | GPU | Training Time | Storage |
|-----------------|-----|---------------|---------|
| **Small-scale** (models up to 50M params) | RTX 3080/3090 | 1-5 hours per model | 10GB |
| **Medium-scale** (models up to 500M params) | A100 40GB | 5-20 hours per model | 50GB |
| **Large-scale** (models 1B+ params) | A100 80GB or multi-GPU | 1-3 days per model | 200GB+ |

### Recommended Experiment Grid

**Minimal Grid** (for quick iteration):
- Model sizes: 10M, 30M, 100M params
- Dataset sizes: 1M, 10M, 100M tokens
- Total experiments: 9 runs
- Estimated time: 5-10 hours on A100

**Comprehensive Grid** (for detailed analysis):
- Model sizes: 10M, 30M, 100M, 300M, 1B params
- Dataset sizes: 1M, 10M, 100M, 1B, 10B tokens
- Multiple seeds: 3 runs per configuration
- Total experiments: 75 runs
- Estimated time: 3-7 days on A100

### Compute Budget Estimation

For a single training run:
- **FLOPs**: â‰ˆ 6 Ã— N Ã— D (forward + backward)
  - N = number of parameters
  - D = number of tokens
- **Example**: 100M param model on 1B tokens â‰ˆ 6 Ã— 10^8 Ã— 10^9 = 6 Ã— 10^17 FLOPs
- **A100 throughput**: ~300 TFLOPS â†’ ~33 minutes

## Important Notes

### âš ï¸ Implementation Limitations

1. **No Official API Access**: This assignment originally required access to a proprietary training API for running large-scale experiments. Since that API is unavailable:
   - **Alternative 1**: Use publicly available scaling law datasets/papers
   - **Alternative 2**: Run smaller-scale experiments with custom training scripts
   - **Alternative 3**: Use synthetic data for proof-of-concept analysis

2. **Verification Challenges**: Without official test cases, verify your implementation by:
   - Comparing trends with published scaling law papers (e.g., Chinchilla, Kaplan et al.)
   - Ensuring power-law fits make sense (exponents typically Î± â‰ˆ 0.05-0.15)
   - Checking that larger models consistently achieve lower loss

3. **Resource Intensive**: Thorough scaling experiments require significant compute:
   - Consider starting with smaller models and datasets
   - Use learning rate sweeps to find optimal hyperparameters quickly
   - Leverage checkpointing to resume interrupted experiments

### ğŸ’¡ Tips for Success

1. **Start Small**: Begin with a 3x3 grid (3 model sizes Ã— 3 dataset sizes) to validate your setup.

2. **Fix Hyperparameters**: Keep learning rate, batch size, and other hyperparameters constant across experiments to isolate scaling effects.

3. **Use Log Scales**: Plot results on log-log axes to visualize power laws clearly.

4. **Track Everything**: Log all hyperparameters, random seeds, and environment details for reproducibility.

5. **Leverage Published Data**: Papers like Chinchilla and Kaplan et al. provide reference datasets you can use to validate your analysis methods.

6. **Automate Experiments**: Write scripts to automatically run the full experiment grid and collect results.

### ğŸ” Expected Observations

Based on scaling law research, you should observe:

1. **Model Size Scaling**: Loss decreases as a power law with model size:
   - L(N) âˆ N^(-Î±) where Î± â‰ˆ 0.05-0.15
   - Larger models consistently outperform smaller ones (given enough data)

2. **Data Size Scaling**: Loss decreases with dataset size:
   - L(D) âˆ D^(-Î²) where Î² â‰ˆ 0.05-0.10
   - More data helps, but with diminishing returns

3. **Compute-Optimal Training** (Chinchilla Laws):
   - For a fixed compute budget C, optimal allocation scales both N and D with C
   - N_opt âˆ C^0.50, D_opt âˆ C^0.50
   - Training "small model for longer" is suboptimal compared to compute-optimal allocation

4. **Smooth Scaling**: Performance should scale smoothly without sudden jumps or drops.

### ğŸ“Š Analysis Deliverables

For this assignment, you typically need to produce:

1. **Scaling Curves**: Plots showing L(N), L(D), L(C) with fitted power laws
2. **Compute-Optimal Frontier**: Pareto frontier of best models at each compute budget
3. **Predictions**: Extrapolated performance for larger scales
4. **Report**: Analysis of findings, comparison with published literature

### ğŸ”— Reference Materials

**Key Papers**:
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) - Kaplan et al., 2020
- [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) - Chinchilla paper, 2022
- [Scaling Laws for Autoregressive Generative Modeling](https://arxiv.org/abs/2010.14701) - Henighan et al., 2020

**Tools**:
- [WandB](https://wandb.ai/) - Experiment tracking
- [Matplotlib/Seaborn](https://matplotlib.org/) - Visualization
- [SciPy](https://scipy.org/) - Curve fitting

## Assignment Handout

For detailed assignment requirements and theoretical background, see:
- [cs336_spring2025_assignment3_scaling.pdf](./cs336_spring2025_assignment3_scaling.pdf)

## License

This code is provided for educational purposes as part of Stanford CS336.

---

# ä¸­æ–‡ç‰ˆæœ¬ | Chinese Version

# CS336 ä½œä¸š 3: ç¼©æ”¾å®šå¾‹

æœ¬ä½œä¸šæ¢è®¨è¯­è¨€æ¨¡å‹ä¸­çš„å®è¯ç¼©æ”¾å®šå¾‹ï¼Œç ”ç©¶æ¨¡å‹æ€§èƒ½å¦‚ä½•éšè®¡ç®—èµ„æºã€æ¨¡å‹å¤§å°å’Œæ•°æ®é›†å¤§å°å˜åŒ–ã€‚ç›®æ ‡æ˜¯ç†è§£å’Œé¢„æµ‹ä¸åŒè§„æ¨¡ä¸‹çš„è¯­è¨€æ¨¡å‹è¡Œä¸ºã€‚

## ğŸ“‹ ç›®å½•
- [æ¦‚è¿°](#æ¦‚è¿°-1)
- [å®ç°ç»†èŠ‚](#å®ç°ç»†èŠ‚-1)
- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„-1)
- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®-1)
- [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—-1)
- [è®¡ç®—èµ„æºéœ€æ±‚](#è®¡ç®—èµ„æºéœ€æ±‚-1)
- [é‡è¦è¯´æ˜](#é‡è¦è¯´æ˜-1)

## æ¦‚è¿°

ç¼©æ”¾å®šå¾‹æè¿°äº†è¯­è¨€æ¨¡å‹æ€§èƒ½ï¼ˆé€šè¿‡æŸå¤±æˆ–å›°æƒ‘åº¦è¡¡é‡ï¼‰å¦‚ä½•éšä»¥ä¸‹å› ç´ å˜åŒ–ï¼š

1. **æ¨¡å‹å¤§å°** (N): éåµŒå…¥å‚æ•°æ•°é‡
2. **æ•°æ®é›†å¤§å°** (D): è®­ç»ƒ token æ•°é‡
3. **è®¡ç®—é¢„ç®—** (C): è®­ç»ƒä½¿ç”¨çš„æ€»æµ®ç‚¹è¿ç®—æ¬¡æ•°

æœ¬ä½œä¸šåŒ…æ‹¬ï¼š
- åœ¨ä¸åŒæ•°æ®é›†å¤§å°ä¸Šè®­ç»ƒå„ç§è§„æ¨¡çš„æ¨¡å‹
- æµ‹é‡æœ€ç»ˆæŸå¤±å’Œè®­ç»ƒåŠ¨æ€
- å°†å¹‚å¾‹å…³ç³»æ‹Ÿåˆåˆ°å®è¯æ•°æ®
- é¢„æµ‹ç»™å®šè®¡ç®—é¢„ç®—ä¸‹çš„æœ€ä¼˜æ¨¡å‹é…ç½®
- ç†è§£è®¡ç®—æœ€ä¼˜è®­ç»ƒï¼ˆChinchilla ç¼©æ”¾å®šå¾‹ï¼‰

## å®ç°ç»†èŠ‚

### æ ¸å¿ƒç»„ä»¶

#### 1. Transformer è¯­è¨€æ¨¡å‹ (`cs336_scaling/model.py`)

æ ‡å‡†çš„ Transformer è§£ç å™¨å®ç°ï¼ŒåŒ…å«ï¼š
- **Token å’Œä½ç½®åµŒå…¥**: Token å’Œä½ç½®çš„å¯å­¦ä¹ åµŒå…¥
- **å¤šå¤´è‡ªæ³¨æ„åŠ›**: å¸¦ dropout çš„å› æœæ³¨æ„åŠ›
- **å‰é¦ˆç½‘ç»œ**: GELU æ¿€æ´»çš„ FFN
- **å±‚å½’ä¸€åŒ–**: Pre-norm æ¶æ„
- **æ–‡æœ¬ç”Ÿæˆ**: æ”¯æŒæ¸©åº¦/top-k çš„è‡ªå›å½’é‡‡æ ·

**å…³é”®æ–¹æ³•**:
- `forward()`: æ ‡å‡†å‰å‘ä¼ æ’­ï¼Œè¿”å› logits
- `generate()`: è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆ
- `from_pretrained()`: åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
- `get_num_params()`: ç»Ÿè®¡éåµŒå…¥å‚æ•°æ•°é‡

#### 2. ç¼©æ”¾å®šå¾‹å®éªŒ

æœ¬ä½œä¸šéœ€è¦è¿è¡Œç³»ç»Ÿæ€§å®éªŒä»¥æ”¶é›†æ•°æ®ç‚¹ï¼š

**å®éªŒè®¾è®¡**:
- è®­ç»ƒä¸åŒå±‚æ•°/ç»´åº¦çš„æ¨¡å‹
- æ”¹å˜æ•°æ®é›†å¤§å°ï¼ˆä¾‹å¦‚ï¼Œ1Mã€10Mã€100Mã€1B tokenï¼‰
- åœ¨ä¸åŒè®­ç»ƒæ­¥éª¤æµ‹é‡æŸå¤±
- æ”¶é›†è®¡ç®—æŒ‡æ ‡ï¼ˆFLOPsã€è®­ç»ƒæ—¶é—´ï¼‰

**åˆ†æ**:
- æ‹Ÿåˆå¹‚å¾‹: L(N) = aN^(-Î±), L(D) = bD^(-Î²), L(C) = cC^(-Î³)
- ç¡®å®š N å’Œ D ä¹‹é—´çš„è®¡ç®—æœ€ä¼˜åˆ†é…
- é¢„æµ‹æœªè§é…ç½®çš„æ€§èƒ½

## é¡¹ç›®ç»“æ„

```
assignment3-scaling/
â”œâ”€â”€ cs336_scaling/
â”‚   â”œâ”€â”€ model.py               # Transformer è¯­è¨€æ¨¡å‹å®ç°
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ data/                      # è®­ç»ƒæ•°æ®é›†ï¼ˆç”¨æˆ·æä¾›ï¼‰
â”œâ”€â”€ experiments/               # å®éªŒè„šæœ¬å’Œç»“æœï¼ˆç”¨æˆ·åˆ›å»ºï¼‰
â”œâ”€â”€ cs336_spring2025_assignment3_scaling.pdf  # ä½œä¸šè¯´æ˜
â”œâ”€â”€ pyproject.toml             # ä¾èµ–é¡¹
â”œâ”€â”€ uv.lock                    # é”å®šæ–‡ä»¶
â””â”€â”€ README.md                  # æœ¬æ–‡ä»¶
```

**æ³¨æ„**: æœ¬ä½œä¸šä¸»è¦æ˜¯å®éªŒ/åˆ†ææ€§çš„ã€‚ä½ éœ€è¦è‡ªè¡Œåˆ›å»ºï¼š
- è®­ç»ƒè„šæœ¬
- æ•°æ®å¤„ç†ç®¡é“
- å®éªŒè·Ÿè¸ªä»£ç 
- åˆ†æç¬”è®°æœ¬

## ç¯å¢ƒé…ç½®

### 1. å®‰è£…ä¾èµ–

```bash
# å®‰è£… uvï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
curl -LsSf https://astral.sh/uv/install.sh | sh

# å®‰è£…ä¾èµ–
uv sync
```

### 2. å‡†å¤‡æ•°æ®é›†

ä½ éœ€è¦å„ç§å¤§å°çš„æ•°æ®é›†ã€‚é€‰é¡¹åŒ…æ‹¬ï¼š

**é€‰é¡¹ 1: ä½¿ç”¨å…¬å¼€æ•°æ®é›†**
```bash
mkdir -p data

# TinyStories (~1GB)
wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt -O data/tinystories.txt

# OpenWebText (~40GB)
# ä» https://huggingface.co/datasets/Skylion007/openwebtext ä¸‹è½½

# The Pile (825GB)
# ä» https://pile.eleuther.ai/ ä¸‹è½½
```

**é€‰é¡¹ 2: åˆ›å»ºåˆæˆæ•°æ®é›†**
```python
# ç”Ÿæˆå—æ§å¤§å°çš„æ•°æ®é›†
import random
import string

def generate_synthetic_text(num_tokens, vocab_size=10000):
    """ä¸ºç¼©æ”¾å®éªŒç”Ÿæˆéšæœºæ–‡æœ¬"""
    tokens = [str(random.randint(0, vocab_size-1)) for _ in range(num_tokens)]
    return ' '.join(tokens)

# åˆ›å»º 1Mã€10Mã€100M token çš„æ•°æ®é›†
for size in [1_000_000, 10_000_000, 100_000_000]:
    text = generate_synthetic_text(size)
    with open(f'data/synthetic_{size}.txt', 'w') as f:
        f.write(text)
```

## ä½¿ç”¨æŒ‡å—

### 1. å®šä¹‰æ¨¡å‹é…ç½®

é€šè¿‡æ”¹å˜è¶…å‚æ•°åˆ›å»ºä¸åŒå¤§å°çš„æ¨¡å‹ï¼š

```python
from cs336_scaling.model import BasicsTransformerLM

# å°æ¨¡å‹ (~10M å‚æ•°)
model_small = BasicsTransformerLM(
    vocab_size=10000,
    context_length=256,
    d_model=256,
    num_layers=4,
    num_heads=4,
    d_ff=1024
)

# ä¸­å‹æ¨¡å‹ (~50M å‚æ•°)
model_medium = BasicsTransformerLM(
    vocab_size=10000,
    context_length=512,
    d_model=512,
    num_layers=8,
    num_heads=8,
    d_ff=2048
)

# å¤§æ¨¡å‹ (~200M å‚æ•°)
model_large = BasicsTransformerLM(
    vocab_size=10000,
    context_length=1024,
    d_model=768,
    num_layers=12,
    num_heads=12,
    d_ff=3072
)
```

### 2. è¿è¡Œç¼©æ”¾å®éªŒ

æ”¶é›†ç¼©æ”¾æ•°æ®çš„ç¤ºä¾‹è®­ç»ƒè„šæœ¬ï¼š

```python
import torch
import numpy as np
from cs336_scaling.model import BasicsTransformerLM

def train_and_measure(model, dataset, num_steps, compute_budget):
    """
    è®­ç»ƒæ¨¡å‹å¹¶åœ¨å„ä¸ªæ£€æŸ¥ç‚¹æµ‹é‡æŸå¤±ã€‚

    è¿”å›: åŒ…å«æŸå¤±ã€è®¡ç®—ä½¿ç”¨é‡ã€æœ€ç»ˆæŒ‡æ ‡çš„å­—å…¸
    """
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
    losses = []

    for step in range(num_steps):
        # è®­ç»ƒæ­¥éª¤
        batch = get_batch(dataset)  # ä½ çš„æ•°æ®åŠ è½½é€»è¾‘
        logits = model(batch['input_ids'])
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)),
                               batch['labels'].view(-1))
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        # å®šæœŸè®°å½•æŸå¤±
        if step % 100 == 0:
            losses.append(loss.item())

    return {
        'final_loss': losses[-1],
        'loss_curve': losses,
        'num_params': model.get_num_params(),
        'num_tokens': num_steps * batch_size * context_length
    }

# è¿è¡Œå®éªŒ
results = []
for model_config in [small_config, medium_config, large_config]:
    for dataset_size in [1e6, 1e7, 1e8]:
        model = BasicsTransformerLM(**model_config)
        result = train_and_measure(model, dataset_size, num_steps=10000)
        results.append(result)
```

### 3. åˆ†æç¼©æ”¾å®šå¾‹

å°†å¹‚å¾‹æ‹Ÿåˆåˆ°ä½ çš„å®éªŒæ•°æ®ï¼š

```python
import numpy as np
from scipy.optimize import curve_fit

# å¹‚å¾‹å‡½æ•°: L = a * N^(-alpha)
def power_law(x, a, alpha):
    return a * np.power(x, -alpha)

# æ”¶é›†æ•°æ®ç‚¹
param_counts = [result['num_params'] for result in results]
final_losses = [result['final_loss'] for result in results]

# æ‹Ÿåˆå¹‚å¾‹
params, _ = curve_fit(power_law, param_counts, final_losses)
a, alpha = params

print(f"ç¼©æ”¾å®šå¾‹: L(N) = {a:.4f} * N^(-{alpha:.4f})")

# é¢„æµ‹æœªè§æ¨¡å‹å¤§å°çš„æŸå¤±
new_model_size = 500_000_000  # 500M å‚æ•°
predicted_loss = power_law(new_model_size, a, alpha)
print(f"500M æ¨¡å‹çš„é¢„æµ‹æŸå¤±: {predicted_loss:.4f}")
```

### 4. è®¡ç®—æœ€ä¼˜è®­ç»ƒ

ç¡®å®šç»™å®šè®¡ç®—é¢„ç®—ä¸‹çš„æœ€ä¼˜æ¨¡å‹å¤§å°ï¼š

```python
def compute_optimal_allocation(compute_budget, alpha_N, alpha_D):
    """
    ç»™å®šè®¡ç®—é¢„ç®— C å’Œç¼©æ”¾æŒ‡æ•°ï¼Œ
    æ‰¾åˆ°æœ€ä¼˜çš„ Nï¼ˆå‚æ•°ï¼‰å’Œ Dï¼ˆtokenï¼‰ã€‚

    Chinchilla å‘ç°: N å’Œ D åº”ä¸ C æˆæ¯”ä¾‹ç¼©æ”¾ã€‚
    """
    # Chinchilla: N_opt âˆ C^0.5, D_opt âˆ C^0.5
    N_opt = (compute_budget / constant_factor) ** 0.5
    D_opt = (compute_budget / constant_factor) ** 0.5

    return N_opt, D_opt
```

## è®¡ç®—èµ„æºéœ€æ±‚

### ç¡¬ä»¶è¦æ±‚

| å®éªŒè§„æ¨¡ | GPU | è®­ç»ƒæ—¶é—´ | å­˜å‚¨ |
|-----------------|-----|---------------|---------:|
| **å°è§„æ¨¡** (æœ€å¤š 50M å‚æ•°çš„æ¨¡å‹) | RTX 3080/3090 | æ¯ä¸ªæ¨¡å‹ 1-5 å°æ—¶ | 10GB |
| **ä¸­ç­‰è§„æ¨¡** (æœ€å¤š 500M å‚æ•°çš„æ¨¡å‹) | A100 40GB | æ¯ä¸ªæ¨¡å‹ 5-20 å°æ—¶ | 50GB |
| **å¤§è§„æ¨¡** (1B+ å‚æ•°çš„æ¨¡å‹) | A100 80GB æˆ–å¤š GPU | æ¯ä¸ªæ¨¡å‹ 1-3 å¤© | 200GB+ |

### æ¨èçš„å®éªŒç½‘æ ¼

**æœ€å°ç½‘æ ¼**ï¼ˆç”¨äºå¿«é€Ÿè¿­ä»£ï¼‰:
- æ¨¡å‹å¤§å°: 10Mã€30Mã€100M å‚æ•°
- æ•°æ®é›†å¤§å°: 1Mã€10Mã€100M token
- æ€»å®éªŒæ•°: 9 æ¬¡è¿è¡Œ
- é¢„è®¡æ—¶é—´: åœ¨ A100 ä¸Š 5-10 å°æ—¶

**ç»¼åˆç½‘æ ¼**ï¼ˆç”¨äºè¯¦ç»†åˆ†æï¼‰:
- æ¨¡å‹å¤§å°: 10Mã€30Mã€100Mã€300Mã€1B å‚æ•°
- æ•°æ®é›†å¤§å°: 1Mã€10Mã€100Mã€1Bã€10B token
- å¤šæ¬¡è¿è¡Œ: æ¯ä¸ªé…ç½® 3 æ¬¡
- æ€»å®éªŒæ•°: 75 æ¬¡è¿è¡Œ
- é¢„è®¡æ—¶é—´: åœ¨ A100 ä¸Š 3-7 å¤©

### è®¡ç®—é¢„ç®—ä¼°ç®—

å¯¹äºå•æ¬¡è®­ç»ƒè¿è¡Œï¼š
- **FLOPs**: â‰ˆ 6 Ã— N Ã— Dï¼ˆå‰å‘ + åå‘ï¼‰
  - N = å‚æ•°æ•°é‡
  - D = token æ•°é‡
- **ç¤ºä¾‹**: åœ¨ 1B token ä¸Šè®­ç»ƒ 100M å‚æ•°æ¨¡å‹ â‰ˆ 6 Ã— 10^8 Ã— 10^9 = 6 Ã— 10^17 FLOPs
- **A100 ååé‡**: çº¦ 300 TFLOPS â†’ çº¦ 33 åˆ†é’Ÿ

## é‡è¦è¯´æ˜

### âš ï¸ å®ç°é™åˆ¶

1. **æ— å®˜æ–¹ API è®¿é—®**: æœ¬ä½œä¸šæœ€åˆéœ€è¦è®¿é—®ä¸“æœ‰è®­ç»ƒ API æ¥è¿è¡Œå¤§è§„æ¨¡å®éªŒã€‚ç”±äºè¯¥ API ä¸å¯ç”¨ï¼š
   - **æ›¿ä»£æ–¹æ¡ˆ 1**: ä½¿ç”¨å…¬å¼€å¯ç”¨çš„ç¼©æ”¾å®šå¾‹æ•°æ®é›†/è®ºæ–‡
   - **æ›¿ä»£æ–¹æ¡ˆ 2**: ä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒè„šæœ¬è¿è¡Œè¾ƒå°è§„æ¨¡çš„å®éªŒ
   - **æ›¿ä»£æ–¹æ¡ˆ 3**: ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¦‚å¿µéªŒè¯åˆ†æ

2. **éªŒè¯æŒ‘æˆ˜**: æ²¡æœ‰å®˜æ–¹æµ‹è¯•ç”¨ä¾‹ï¼Œå¯é€šè¿‡ä»¥ä¸‹æ–¹å¼éªŒè¯ä½ çš„å®ç°ï¼š
   - å°†è¶‹åŠ¿ä¸å·²å‘è¡¨çš„ç¼©æ”¾å®šå¾‹è®ºæ–‡ï¼ˆä¾‹å¦‚ Chinchillaã€Kaplan ç­‰ï¼‰è¿›è¡Œæ¯”è¾ƒ
   - ç¡®ä¿å¹‚å¾‹æ‹Ÿåˆåˆç†ï¼ˆæŒ‡æ•°é€šå¸¸ Î± â‰ˆ 0.05-0.15ï¼‰
   - æ£€æŸ¥è¾ƒå¤§æ¨¡å‹æ˜¯å¦å§‹ç»ˆè·å¾—è¾ƒä½æŸå¤±

3. **èµ„æºå¯†é›†**: å…¨é¢çš„ç¼©æ”¾å®éªŒéœ€è¦å¤§é‡è®¡ç®—ï¼š
   - è€ƒè™‘ä»è¾ƒå°çš„æ¨¡å‹å’Œæ•°æ®é›†å¼€å§‹
   - ä½¿ç”¨å­¦ä¹ ç‡æ‰«æå¿«é€Ÿæ‰¾åˆ°æœ€ä¼˜è¶…å‚æ•°
   - åˆ©ç”¨æ£€æŸ¥ç‚¹æ¢å¤ä¸­æ–­çš„å®éªŒ

### ğŸ’¡ æˆåŠŸæŠ€å·§

1. **ä»å°å¤„å¼€å§‹**: ä» 3x3 ç½‘æ ¼ï¼ˆ3 ç§æ¨¡å‹å¤§å° Ã— 3 ç§æ•°æ®é›†å¤§å°ï¼‰å¼€å§‹éªŒè¯ä½ çš„è®¾ç½®ã€‚

2. **å›ºå®šè¶…å‚æ•°**: åœ¨å®éªŒä¸­ä¿æŒå­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°å’Œå…¶ä»–è¶…å‚æ•°ä¸å˜ï¼Œä»¥éš”ç¦»ç¼©æ”¾æ•ˆåº”ã€‚

3. **ä½¿ç”¨å¯¹æ•°åˆ»åº¦**: åœ¨å¯¹æ•°-å¯¹æ•°åæ ‡è½´ä¸Šç»˜åˆ¶ç»“æœï¼Œä»¥æ¸…æ™°åœ°å¯è§†åŒ–å¹‚å¾‹ã€‚

4. **è·Ÿè¸ªä¸€åˆ‡**: è®°å½•æ‰€æœ‰è¶…å‚æ•°ã€éšæœºç§å­å’Œç¯å¢ƒè¯¦ç»†ä¿¡æ¯ä»¥ç¡®ä¿å¯é‡å¤æ€§ã€‚

5. **åˆ©ç”¨å·²å‘è¡¨çš„æ•°æ®**: Chinchilla å’Œ Kaplan ç­‰è®ºæ–‡æä¾›äº†å¯ç”¨äºéªŒè¯åˆ†ææ–¹æ³•çš„å‚è€ƒæ•°æ®é›†ã€‚

6. **è‡ªåŠ¨åŒ–å®éªŒ**: ç¼–å†™è„šæœ¬è‡ªåŠ¨è¿è¡Œå®Œæ•´çš„å®éªŒç½‘æ ¼å¹¶æ”¶é›†ç»“æœã€‚

### ğŸ” é¢„æœŸè§‚å¯Ÿ

åŸºäºç¼©æ”¾å®šå¾‹ç ”ç©¶ï¼Œä½ åº”è¯¥è§‚å¯Ÿåˆ°ï¼š

1. **æ¨¡å‹å¤§å°ç¼©æ”¾**: æŸå¤±éšæ¨¡å‹å¤§å°å‘ˆå¹‚å¾‹ä¸‹é™ï¼š
   - L(N) âˆ N^(-Î±)ï¼Œå…¶ä¸­ Î± â‰ˆ 0.05-0.15
   - è¾ƒå¤§æ¨¡å‹å§‹ç»ˆä¼˜äºè¾ƒå°æ¨¡å‹ï¼ˆç»™å®šè¶³å¤Ÿçš„æ•°æ®ï¼‰

2. **æ•°æ®å¤§å°ç¼©æ”¾**: æŸå¤±éšæ•°æ®é›†å¤§å°ä¸‹é™ï¼š
   - L(D) âˆ D^(-Î²)ï¼Œå…¶ä¸­ Î² â‰ˆ 0.05-0.10
   - æ›´å¤šæ•°æ®æœ‰å¸®åŠ©ï¼Œä½†æ”¶ç›Šé€’å‡

3. **è®¡ç®—æœ€ä¼˜è®­ç»ƒ**ï¼ˆChinchilla å®šå¾‹ï¼‰:
   - å¯¹äºå›ºå®šçš„è®¡ç®—é¢„ç®— Cï¼Œæœ€ä¼˜åˆ†é…ä½¿ N å’Œ D éƒ½éš C ç¼©æ”¾
   - N_opt âˆ C^0.50, D_opt âˆ C^0.50
   - "é•¿æ—¶é—´è®­ç»ƒå°æ¨¡å‹"ç›¸æ¯”è®¡ç®—æœ€ä¼˜åˆ†é…æ˜¯æ¬¡ä¼˜çš„

4. **å¹³æ»‘ç¼©æ”¾**: æ€§èƒ½åº”å¹³æ»‘ç¼©æ”¾ï¼Œæ²¡æœ‰çªç„¶çš„è·³è·ƒæˆ–ä¸‹é™ã€‚

### ğŸ“Š åˆ†æäº¤ä»˜æˆæœ

å¯¹äºæœ¬ä½œä¸šï¼Œä½ é€šå¸¸éœ€è¦äº§å‡ºï¼š

1. **ç¼©æ”¾æ›²çº¿**: æ˜¾ç¤º L(N)ã€L(D)ã€L(C) åŠæ‹Ÿåˆå¹‚å¾‹çš„å›¾è¡¨
2. **è®¡ç®—æœ€ä¼˜å‰æ²¿**: æ¯ä¸ªè®¡ç®—é¢„ç®—ä¸‹æœ€ä½³æ¨¡å‹çš„ Pareto å‰æ²¿
3. **é¢„æµ‹**: å¯¹æ›´å¤§è§„æ¨¡çš„å¤–æ¨æ€§èƒ½
4. **æŠ¥å‘Š**: åˆ†æå‘ç°ï¼Œä¸å·²å‘è¡¨æ–‡çŒ®çš„æ¯”è¾ƒ

### ğŸ”— å‚è€ƒèµ„æ–™

**å…³é”®è®ºæ–‡**:
- [ç¥ç»è¯­è¨€æ¨¡å‹çš„ç¼©æ”¾å®šå¾‹](https://arxiv.org/abs/2001.08361) - Kaplan ç­‰ï¼Œ2020
- [è®­ç»ƒè®¡ç®—æœ€ä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹](https://arxiv.org/abs/2203.15556) - Chinchilla è®ºæ–‡ï¼Œ2022
- [è‡ªå›å½’ç”Ÿæˆå»ºæ¨¡çš„ç¼©æ”¾å®šå¾‹](https://arxiv.org/abs/2010.14701) - Henighan ç­‰ï¼Œ2020

**å·¥å…·**:
- [WandB](https://wandb.ai/) - å®éªŒè·Ÿè¸ª
- [Matplotlib/Seaborn](https://matplotlib.org/) - å¯è§†åŒ–
- [SciPy](https://scipy.org/) - æ›²çº¿æ‹Ÿåˆ

## ä½œä¸šè¯´æ˜

è¯¦ç»†çš„ä½œä¸šè¦æ±‚å’Œç†è®ºèƒŒæ™¯ï¼Œè¯·å‚é˜…ï¼š
- [cs336_spring2025_assignment3_scaling.pdf](./cs336_spring2025_assignment3_scaling.pdf)

## è®¸å¯è¯

æœ¬ä»£ç ä»…ä¾›æ•™è‚²ç›®çš„ä½¿ç”¨ï¼Œæ˜¯æ–¯å¦ç¦ CS336 è¯¾ç¨‹çš„ä¸€éƒ¨åˆ†ã€‚
