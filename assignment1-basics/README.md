# CS336 Assignment 1: Language Modeling Basics

This assignment implements the foundational components of a Transformer-based language model from scratch, including tokenization, model architecture, training infrastructure, and text generation.

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Implementation Details](#implementation-details)
- [Project Structure](#project-structure)
- [Setup](#setup)
- [Usage](#usage)
- [Testing](#testing)
- [Computational Requirements](#computational-requirements)
- [Important Notes](#important-notes)

## Overview

This assignment covers the fundamental building blocks of language modeling:

1. **Tokenization**: Byte Pair Encoding (BPE) implementation
2. **Model Architecture**: Multi-layer Transformer decoder with:
   - Multi-head self-attention with RoPE (Rotary Position Embeddings)
   - Feed-forward networks with SwiGLU activation
   - Layer normalization (RMSNorm)
3. **Training Infrastructure**:
   - AdamW optimizer with weight decay
   - Learning rate scheduling (warmup + cosine decay)
   - Gradient clipping
   - Checkpointing
4. **Text Generation**: Auto-regressive sampling with temperature control

## Implementation Details

### Key Components

#### 1. Tokenizer (`cs336_basics/tokenizer.py`)
- **BPE Training**: Implements byte-pair encoding from scratch
- **Pre-tokenization**: Supports GPT-4 style regex-based pre-tokenization
- **Special Tokens**: Handles `<|endoftext|>` and other special tokens
- **Encoding/Decoding**: Efficient token sequence conversion

**Note**: âš ï¸ CPU parallel tokenizer training is **not implemented**. Training runs on a single process.

#### 2. Transformer Model (`cs336_basics/model/`)
- **Attention**: Multi-head self-attention with causal masking
- **Position Encoding**: RoPE (Rotary Position Embeddings)
- **Activation**: SwiGLU activation function
- **Normalization**: RMSNorm for improved training stability

#### 3. Training (`cs336_basics/train.py`)
- **Optimizer**: Custom AdamW implementation
- **LR Schedule**: Linear warmup + cosine annealing
- **Monitoring**: Integrated W&B logging and tqdm progress bars
- **Checkpointing**: Automatic model checkpointing with resume capability

#### 4. Generation (`cs336_basics/generate.py`)
- Temperature-controlled sampling
- Top-k and top-p (nucleus) sampling support
- Batch generation capabilities

## Project Structure

```
assignment1-basics/
â”œâ”€â”€ cs336_basics/              # Main implementation module
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ modules.py         # Attention, FFN, LayerNorm modules
â”‚   â”‚   â””â”€â”€ transformer.py     # Full Transformer LM
â”‚   â”œâ”€â”€ trainer/
â”‚   â”‚   â”œâ”€â”€ AdamW.py          # AdamW optimizer
â”‚   â”‚   â”œâ”€â”€ data_loading.py   # Data loading utilities
â”‚   â”‚   â””â”€â”€ utils.py          # Training utilities (loss, lr schedule, clipping)
â”‚   â”œâ”€â”€ tokenizer.py          # BPE tokenizer implementation
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â”œâ”€â”€ generate.py           # Text generation utilities
â”‚   â””â”€â”€ check_pointing.py     # Checkpoint save/load
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_bpe.py          # Script to train BPE tokenizer
â”‚   â””â”€â”€ tokenize_test.py      # Tokenizer testing script
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ adapters.py           # Test adapters (IMPORTANT: Connect your implementation here)
â”‚   â”œâ”€â”€ test_tokenizer.py    # Tokenizer tests
â”‚   â”œâ”€â”€ test_model.py         # Model architecture tests
â”‚   â”œâ”€â”€ test_optimizer.py    # Optimizer tests
â”‚   â””â”€â”€ ...                   # Other test files
â”œâ”€â”€ tokenizer/                # Saved tokenizer files (generated)
â”œâ”€â”€ checkpoints/              # Model checkpoints (generated)
â”œâ”€â”€ pyproject.toml            # Project dependencies
â””â”€â”€ README.md                 # This file
```

## Setup

### 1. Install Dependencies

This project uses `uv` for dependency management:

```bash
# Install uv if you haven't already
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install dependencies (automatic with uv run)
uv sync
```

### 2. Download Training Data

Download the TinyStories and OpenWebText datasets:

```bash
mkdir -p data
cd data

# TinyStories dataset
wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt
wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt

# OpenWebText sample
wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_train.txt.gz
gunzip owt_train.txt.gz
wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_valid.txt.gz
gunzip owt_valid.txt.gz

cd ..
```

## Usage

### Step 1: Train BPE Tokenizer

First, train a BPE tokenizer on your corpus:

```bash
uv run python scripts/train_bpe.py
```

This will:
- Train a BPE tokenizer with vocab size 10,000 on TinyStories
- Save vocabulary and merge rules to `tokenizer/` directory
- Takes approximately **5-15 minutes** on CPU (single-threaded)

**Configuration** (edit `scripts/train_bpe.py`):
- `vocab_size`: Target vocabulary size (default: 10,000)
- `special_tokens`: List of special tokens (default: `["<|endoftext|>"]`)
- `INPUT_PATH`: Path to training corpus

### Step 2: Tokenize Data

After training the tokenizer, convert your text data to token sequences:

```bash
# Create tokenized .dat files from text
uv run python scripts/tokenize_test.py
```

This creates memory-mapped `.dat` files for efficient training:
- `data/train.dat`: Tokenized training data
- `data/valid.dat`: Tokenized validation data

### Step 3: Train Language Model

Train the Transformer language model:

```bash
# Basic training (with W&B logging)
uv run python cs336_basics/train.py \
    --data_dir ./data \
    --wandb_project "cs336-basics" \
    --wandb_run_name "tinystories-baseline"

# Training without W&B
uv run python cs336_basics/train.py \
    --data_dir ./data \
    --no_wandb

# Custom configuration
uv run python cs336_basics/train.py \
    --data_dir ./data \
    --d_model 512 \
    --num_layers 6 \
    --num_heads 8 \
    --batch_size 64 \
    --train_steps 10000 \
    --max_lr 3e-4 \
    --no_wandb
```

**Key Training Arguments**:
- Model: `--d_model`, `--num_layers`, `--num_heads`, `--d_ff`
- Training: `--batch_size`, `--train_steps`, `--max_lr`, `--weight_decay`
- Monitoring: `--val_interval`, `--save_intervals`, `--log_intervals`
- Checkpointing: `--save_ckp_path`, `--resume_ckp`

**Resume from Checkpoint**:
```bash
uv run python cs336_basics/train.py \
    --data_dir ./data \
    --resume_ckp ./checkpoints/checkpoint_5000.pt
```

### Step 4: Generate Text

Generate text using a trained model:

```python
from cs336_basics.generate import generate_text
from cs336_basics.model.transformer import transformer_lm
import torch

# Load model
model = transformer_lm(vocab_size=10000, ...)
checkpoint = torch.load('checkpoints/checkpoint_final.pt')
model.load_state_dict(checkpoint['model_state_dict'])

# Generate
prompt = "Once upon a time"
generated = generate_text(model, tokenizer, prompt, max_length=100, temperature=0.8)
print(generated)
```

## Testing

Run the comprehensive test suite:

```bash
# Run all tests
uv run pytest

# Run specific test modules
uv run pytest tests/test_tokenizer.py
uv run pytest tests/test_model.py
uv run pytest tests/test_optimizer.py

# Run with verbose output
uv run pytest -v
```

**Important**: Before running tests, you must complete the adapter functions in `tests/adapters.py`. This file connects your implementation to the test suite.

### Test Coverage

- âœ… Tokenizer: BPE training, encoding, decoding, special tokens
- âœ… Model: Attention, FFN, layer norm, full transformer forward pass
- âœ… Optimizer: AdamW correctness, weight decay, learning rate
- âœ… Training Utils: Cross-entropy, gradient clipping, LR scheduling
- âœ… Serialization: Checkpoint save/load

## Computational Requirements

### Hardware Recommendations

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| **GPU** | None (CPU works) | NVIDIA GPU with 8GB+ VRAM |
| **RAM** | 8GB | 16GB+ |
| **Storage** | 10GB | 20GB+ |

### Training Time Estimates

**Tokenizer Training** (BPE on TinyStories, vocab_size=10,000):
- CPU (single-thread): ~10-15 minutes
- Note: Parallel training not implemented

**Model Training** (Default config: 4 layers, d_model=512, 6K steps):
- **CPU**: ~8-12 hours (not recommended)
- **Apple M1/M2 (MPS)**: ~2-3 hours
- **NVIDIA RTX 3090**: ~45-60 minutes
- **NVIDIA A100**: ~20-30 minutes

**Larger Model** (6 layers, d_model=768, 20K steps):
- **RTX 3090**: ~3-4 hours
- **A100**: ~1.5-2 hours

### Memory Usage

- **Small Model** (4 layers, d_model=512, batch_size=32): ~2-3GB GPU memory
- **Medium Model** (6 layers, d_model=768, batch_size=64): ~6-8GB GPU memory
- **Large Model** (12 layers, d_model=1024, batch_size=64): ~12-16GB GPU memory

**Tip**: Reduce `batch_size` if you encounter OOM errors.

## Important Notes

### âš ï¸ Implementation Limitations

1. **No CPU Parallelization**: The BPE tokenizer training runs on a single CPU thread. Parallel training is not implemented.

2. **Memory-Mapped Data**: Training uses `np.memmap` for efficient data loading. Ensure `.dat` files are created before training.

3. **Device Compatibility**: Supports CUDA, MPS (Apple Silicon), and CPU. Auto-detection available with `--device auto`.

### ğŸ’¡ Tips for Success

1. **Start Small**: Begin with the default configuration (4 layers, d_model=512) to verify everything works.

2. **Monitor Training**: Use W&B (`--wandb_project`) to track loss curves and learning rate schedules.

3. **Validate Frequently**: Set `--val_interval=100` to catch training issues early.

4. **Checkpoint Often**: Use `--save_intervals=1000` to avoid losing progress.

5. **Hyperparameter Tuning**:
   - Learning rate is critical: try `[1e-4, 3e-4, 1e-3]`
   - Warmup helps: use `--warm_up_it=500` for stable training
   - Gradient clipping prevents explosions: keep `--clip_grad_norm=1.0`

6. **Test First**: Run `uv run pytest` to ensure your implementation is correct before long training runs.

### ğŸ” Debugging

**Tests Failing?**
- Check `tests/adapters.py` - all adapter functions must be implemented
- Ensure tokenizer is trained and saved to `tokenizer/` directory
- Verify data files exist in `data/` directory

**Training Issues?**
- Loss = NaN: Lower learning rate or increase gradient clipping
- Loss not decreasing: Check data loading, verify tokenization
- OOM errors: Reduce `--batch_size` or `--context_len`

**Slow Training?**
- Verify GPU is being used: check device output at start
- For Apple Silicon: ensure MPS backend is available (`torch.backends.mps.is_available()`)
- Reduce model size or batch size for faster iteration

## Assignment Handout

For detailed assignment requirements and theoretical background, see:
- [cs336_spring2025_assignment1_basics.pdf](./cs336_spring2025_assignment1_basics.pdf)

## Additional Resources

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer paper
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) - RoPE explanation
- [GLU Variants Improve Transformer](https://arxiv.org/abs/2002.05202) - SwiGLU activation

## License

This code is provided for educational purposes as part of Stanford CS336.

---

# ä¸­æ–‡ç‰ˆæœ¬ | Chinese Version

# CS336 ä½œä¸š1ï¼šè¯­è¨€å»ºæ¨¡åŸºç¡€

æœ¬ä½œä¸šä»é›¶å®ç°åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹çš„åŸºç¡€ç»„ä»¶ï¼ŒåŒ…æ‹¬åˆ†è¯ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒåŸºç¡€è®¾æ–½å’Œæ–‡æœ¬ç”Ÿæˆã€‚

## ğŸ“‹ ç›®å½•
- [æ¦‚è¿°](#æ¦‚è¿°-1)
- [å®ç°ç»†èŠ‚](#å®ç°ç»†èŠ‚-1)
- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„-1)
- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®-1)
- [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—-1)
- [æµ‹è¯•](#æµ‹è¯•-1)
- [è®¡ç®—èµ„æºéœ€æ±‚](#è®¡ç®—èµ„æºéœ€æ±‚-1)
- [é‡è¦è¯´æ˜](#é‡è¦è¯´æ˜-1)

## æ¦‚è¿°

æœ¬ä½œä¸šæ¶µç›–è¯­è¨€å»ºæ¨¡çš„åŸºç¡€æ„å»ºå—ï¼š

1. **åˆ†è¯**ï¼šå­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰å®ç°
2. **æ¨¡å‹æ¶æ„**ï¼šå¤šå±‚Transformerè§£ç å™¨ï¼ŒåŒ…å«ï¼š
   - å¸¦RoPEï¼ˆæ—‹è½¬ä½ç½®åµŒå…¥ï¼‰çš„å¤šå¤´è‡ªæ³¨æ„åŠ›
   - å¸¦SwiGLUæ¿€æ´»çš„å‰é¦ˆç½‘ç»œ
   - å±‚å½’ä¸€åŒ–ï¼ˆRMSNormï¼‰
3. **è®­ç»ƒåŸºç¡€è®¾æ–½**ï¼š
   - å¸¦æƒé‡è¡°å‡çš„AdamWä¼˜åŒ–å™¨
   - å­¦ä¹ ç‡è°ƒåº¦ï¼ˆé¢„çƒ­+ä½™å¼¦è¡°å‡ï¼‰
   - æ¢¯åº¦è£å‰ª
   - æ£€æŸ¥ç‚¹ä¿å­˜
4. **æ–‡æœ¬ç”Ÿæˆ**ï¼šå¸¦æ¸©åº¦æ§åˆ¶çš„è‡ªå›å½’é‡‡æ ·

## å®ç°ç»†èŠ‚

### æ ¸å¿ƒç»„ä»¶

#### 1. åˆ†è¯å™¨ (`cs336_basics/tokenizer.py`)
- **BPEè®­ç»ƒ**ï¼šä»é›¶å®ç°å­—èŠ‚å¯¹ç¼–ç 
- **é¢„åˆ†è¯**ï¼šæ”¯æŒGPT-4é£æ ¼çš„æ­£åˆ™è¡¨è¾¾å¼é¢„åˆ†è¯
- **ç‰¹æ®Štoken**ï¼šå¤„ç†`<|endoftext|>`ç­‰ç‰¹æ®Štoken
- **ç¼–ç /è§£ç **ï¼šé«˜æ•ˆçš„tokenåºåˆ—è½¬æ¢

**æ³¨æ„**ï¼šâš ï¸ **æœªå®ç°**CPUå¹¶è¡Œåˆ†è¯å™¨è®­ç»ƒã€‚è®­ç»ƒåœ¨å•è¿›ç¨‹ä¸Šè¿è¡Œã€‚

#### 2. Transformeræ¨¡å‹ (`cs336_basics/model/`)
- **æ³¨æ„åŠ›**ï¼šå¸¦å› æœæ©ç çš„å¤šå¤´è‡ªæ³¨æ„åŠ›
- **ä½ç½®ç¼–ç **ï¼šRoPEï¼ˆæ—‹è½¬ä½ç½®åµŒå…¥ï¼‰
- **æ¿€æ´»å‡½æ•°**ï¼šSwiGLUæ¿€æ´»å‡½æ•°
- **å½’ä¸€åŒ–**ï¼šRMSNormæå‡è®­ç»ƒç¨³å®šæ€§

#### 3. è®­ç»ƒ (`cs336_basics/train.py`)
- **ä¼˜åŒ–å™¨**ï¼šè‡ªå®šä¹‰AdamWå®ç°
- **å­¦ä¹ ç‡è°ƒåº¦**ï¼šçº¿æ€§é¢„çƒ­+ä½™å¼¦é€€ç«
- **ç›‘æ§**ï¼šé›†æˆW&Bæ—¥å¿—å’Œtqdmè¿›åº¦æ¡
- **æ£€æŸ¥ç‚¹**ï¼šè‡ªåŠ¨æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜å’Œæ¢å¤

#### 4. ç”Ÿæˆ (`cs336_basics/generate.py`)
- æ¸©åº¦æ§åˆ¶é‡‡æ ·
- Top-kå’Œtop-pï¼ˆnucleusï¼‰é‡‡æ ·æ”¯æŒ
- æ‰¹é‡ç”Ÿæˆèƒ½åŠ›

## é¡¹ç›®ç»“æ„

```
assignment1-basics/
â”œâ”€â”€ cs336_basics/              # ä¸»è¦å®ç°æ¨¡å—
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ modules.py         # æ³¨æ„åŠ›ã€FFNã€LayerNormæ¨¡å—
â”‚   â”‚   â””â”€â”€ transformer.py     # å®Œæ•´Transformer LM
â”‚   â”œâ”€â”€ trainer/
â”‚   â”‚   â”œâ”€â”€ AdamW.py          # AdamWä¼˜åŒ–å™¨
â”‚   â”‚   â”œâ”€â”€ data_loading.py   # æ•°æ®åŠ è½½å·¥å…·
â”‚   â”‚   â””â”€â”€ utils.py          # è®­ç»ƒå·¥å…·ï¼ˆæŸå¤±ã€å­¦ä¹ ç‡è°ƒåº¦ã€è£å‰ªï¼‰
â”‚   â”œâ”€â”€ tokenizer.py          # BPEåˆ†è¯å™¨å®ç°
â”‚   â”œâ”€â”€ train.py              # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ generate.py           # æ–‡æœ¬ç”Ÿæˆå·¥å…·
â”‚   â””â”€â”€ check_pointing.py     # æ£€æŸ¥ç‚¹ä¿å­˜/åŠ è½½
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_bpe.py          # BPEåˆ†è¯å™¨è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ tokenize_test.py      # åˆ†è¯å™¨æµ‹è¯•è„šæœ¬
â”œâ”€â”€ tests/                     # æµ‹è¯•ç›®å½•
â”œâ”€â”€ tokenizer/                 # ä¿å­˜çš„åˆ†è¯å™¨æ–‡ä»¶ï¼ˆç”Ÿæˆï¼‰
â”œâ”€â”€ checkpoints/               # æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆç”Ÿæˆï¼‰
â””â”€â”€ README.md                  # æœ¬æ–‡ä»¶
```

## ç¯å¢ƒé…ç½®

### 1. å®‰è£…ä¾èµ–

æœ¬é¡¹ç›®ä½¿ç”¨ `uv` è¿›è¡Œä¾èµ–ç®¡ç†ï¼š

```bash
# å¦‚æœè¿˜æ²¡æœ‰å®‰è£…uv
curl -LsSf https://astral.sh/uv/install.sh | sh

# å®‰è£…ä¾èµ–ï¼ˆä½¿ç”¨uv runæ—¶è‡ªåŠ¨å®‰è£…ï¼‰
uv sync
```

### 2. ä¸‹è½½è®­ç»ƒæ•°æ®

ä¸‹è½½TinyStorieså’ŒOpenWebTextæ•°æ®é›†ï¼š

```bash
mkdir -p data
cd data

# TinyStoriesæ•°æ®é›†
wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt
wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt

# OpenWebTextæ ·æœ¬
wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_train.txt.gz
gunzip owt_train.txt.gz
wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_valid.txt.gz
gunzip owt_valid.txt.gz

cd ..
```

## ä½¿ç”¨æŒ‡å—

### æ­¥éª¤1ï¼šè®­ç»ƒBPEåˆ†è¯å™¨

é¦–å…ˆåœ¨è¯­æ–™åº“ä¸Šè®­ç»ƒBPEåˆ†è¯å™¨ï¼š

```bash
uv run python scripts/train_bpe.py
```

è¿™å°†ï¼š
- åœ¨TinyStoriesä¸Šè®­ç»ƒè¯æ±‡é‡ä¸º10,000çš„BPEåˆ†è¯å™¨
- ä¿å­˜è¯æ±‡è¡¨å’Œåˆå¹¶è§„åˆ™åˆ°`tokenizer/`ç›®å½•
- å¤§çº¦éœ€è¦**5-15åˆ†é’Ÿ**ï¼ˆCPUå•çº¿ç¨‹ï¼‰

### æ­¥éª¤2ï¼šåˆ†è¯æ•°æ®

è®­ç»ƒåˆ†è¯å™¨åï¼Œå°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºtokenåºåˆ—ï¼š

```bash
uv run python scripts/tokenize_test.py
```

è¿™ä¼šåˆ›å»ºå†…å­˜æ˜ å°„çš„`.dat`æ–‡ä»¶ç”¨äºé«˜æ•ˆè®­ç»ƒï¼š
- `data/train.dat`ï¼šåˆ†è¯åçš„è®­ç»ƒæ•°æ®
- `data/valid.dat`ï¼šåˆ†è¯åçš„éªŒè¯æ•°æ®

### æ­¥éª¤3ï¼šè®­ç»ƒè¯­è¨€æ¨¡å‹

è®­ç»ƒTransformerè¯­è¨€æ¨¡å‹ï¼š

```bash
# åŸºç¡€è®­ç»ƒï¼ˆå¸¦W&Bæ—¥å¿—ï¼‰
uv run python cs336_basics/train.py \
    --data_dir ./data \
    --wandb_project "cs336-basics" \
    --wandb_run_name "tinystories-baseline"

# ä¸ä½¿ç”¨W&Bçš„è®­ç»ƒ
uv run python cs336_basics/train.py \
    --data_dir ./data \
    --no_wandb
```

**å…³é”®è®­ç»ƒå‚æ•°**ï¼š
- æ¨¡å‹ï¼š`--d_model`ã€`--num_layers`ã€`--num_heads`ã€`--d_ff`
- è®­ç»ƒï¼š`--batch_size`ã€`--train_steps`ã€`--max_lr`ã€`--weight_decay`
- ç›‘æ§ï¼š`--val_interval`ã€`--save_intervals`ã€`--log_intervals`

## æµ‹è¯•

è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶ï¼š

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
uv run pytest

# è¿è¡Œç‰¹å®šæµ‹è¯•æ¨¡å—
uv run pytest tests/test_tokenizer.py
uv run pytest tests/test_model.py
```

**é‡è¦**ï¼šè¿è¡Œæµ‹è¯•å‰ï¼Œå¿…é¡»å®Œæˆ`tests/adapters.py`ä¸­çš„é€‚é…å™¨å‡½æ•°ã€‚

## è®¡ç®—èµ„æºéœ€æ±‚

### ç¡¬ä»¶æ¨è

| ç»„ä»¶ | æœ€ä½é…ç½® | æ¨èé…ç½® |
|------|---------|---------|
| **GPU** | æ— ï¼ˆCPUå¯è¿è¡Œï¼‰ | NVIDIA GPU 8GB+ VRAM |
| **å†…å­˜** | 8GB | 16GB+ |
| **å­˜å‚¨** | 10GB | 20GB+ |

### è®­ç»ƒæ—¶é—´ä¼°è®¡

**åˆ†è¯å™¨è®­ç»ƒ**ï¼ˆTinyStoriesä¸Šçš„BPEï¼Œè¯æ±‡é‡=10,000ï¼‰ï¼š
- CPUï¼ˆå•çº¿ç¨‹ï¼‰ï¼šçº¦10-15åˆ†é’Ÿ
- æ³¨æ„ï¼šæœªå®ç°å¹¶è¡Œè®­ç»ƒ

**æ¨¡å‹è®­ç»ƒ**ï¼ˆé»˜è®¤é…ç½®ï¼š4å±‚ï¼Œd_model=512ï¼Œ6Kæ­¥ï¼‰ï¼š
- **CPU**ï¼šçº¦8-12å°æ—¶ï¼ˆä¸æ¨èï¼‰
- **Apple M1/M2 (MPS)**ï¼šçº¦2-3å°æ—¶
- **NVIDIA RTX 3090**ï¼šçº¦45-60åˆ†é’Ÿ
- **NVIDIA A100**ï¼šçº¦20-30åˆ†é’Ÿ

### å†…å­˜ä½¿ç”¨

- **å°æ¨¡å‹**ï¼ˆ4å±‚ï¼Œd_model=512ï¼Œbatch_size=32ï¼‰ï¼šçº¦2-3GB GPUå†…å­˜
- **ä¸­æ¨¡å‹**ï¼ˆ6å±‚ï¼Œd_model=768ï¼Œbatch_size=64ï¼‰ï¼šçº¦6-8GB GPUå†…å­˜
- **å¤§æ¨¡å‹**ï¼ˆ12å±‚ï¼Œd_model=1024ï¼Œbatch_size=64ï¼‰ï¼šçº¦12-16GB GPUå†…å­˜

**æç¤º**ï¼šå¦‚é‡OOMé”™è¯¯ï¼Œé™ä½`batch_size`ã€‚

## é‡è¦è¯´æ˜

### âš ï¸ å®ç°é™åˆ¶

1. **æ— CPUå¹¶è¡ŒåŒ–**ï¼šBPEåˆ†è¯å™¨è®­ç»ƒåœ¨å•ä¸ªCPUçº¿ç¨‹ä¸Šè¿è¡Œã€‚æœªå®ç°å¹¶è¡Œè®­ç»ƒã€‚

2. **å†…å­˜æ˜ å°„æ•°æ®**ï¼šè®­ç»ƒä½¿ç”¨`np.memmap`è¿›è¡Œé«˜æ•ˆæ•°æ®åŠ è½½ã€‚è®­ç»ƒå‰ç¡®ä¿åˆ›å»ºäº†`.dat`æ–‡ä»¶ã€‚

3. **è®¾å¤‡å…¼å®¹æ€§**ï¼šæ”¯æŒCUDAã€MPSï¼ˆApple Siliconï¼‰å’ŒCPUã€‚ä½¿ç”¨`--device auto`è‡ªåŠ¨æ£€æµ‹ã€‚

### ğŸ’¡ æˆåŠŸæŠ€å·§

1. **ä»å°å¼€å§‹**ï¼šå…ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆ4å±‚ï¼Œd_model=512ï¼‰éªŒè¯ä¸€åˆ‡æ­£å¸¸ã€‚

2. **ç›‘æ§è®­ç»ƒ**ï¼šä½¿ç”¨W&Bï¼ˆ`--wandb_project`ï¼‰è·Ÿè¸ªæŸå¤±æ›²çº¿å’Œå­¦ä¹ ç‡è°ƒåº¦ã€‚

3. **é¢‘ç¹éªŒè¯**ï¼šè®¾ç½®`--val_interval=100`åŠæ—©å‘ç°è®­ç»ƒé—®é¢˜ã€‚

4. **ç»å¸¸ä¿å­˜æ£€æŸ¥ç‚¹**ï¼šä½¿ç”¨`--save_intervals=1000`é¿å…ä¸¢å¤±è¿›åº¦ã€‚

5. **è¶…å‚æ•°è°ƒä¼˜**ï¼š
   - å­¦ä¹ ç‡è‡³å…³é‡è¦ï¼šå°è¯•`[1e-4, 3e-4, 1e-3]`
   - é¢„çƒ­æœ‰å¸®åŠ©ï¼šä½¿ç”¨`--warm_up_it=500`ç¨³å®šè®­ç»ƒ
   - æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸ï¼šä¿æŒ`--clip_grad_norm=1.0`

### ğŸ” è°ƒè¯•

**æµ‹è¯•å¤±è´¥ï¼Ÿ**
- æ£€æŸ¥`tests/adapters.py` - æ‰€æœ‰é€‚é…å™¨å‡½æ•°å¿…é¡»å®ç°
- ç¡®ä¿åˆ†è¯å™¨å·²è®­ç»ƒå¹¶ä¿å­˜åˆ°`tokenizer/`ç›®å½•
- éªŒè¯æ•°æ®æ–‡ä»¶å­˜åœ¨äº`data/`ç›®å½•

**è®­ç»ƒé—®é¢˜ï¼Ÿ**
- Loss = NaNï¼šé™ä½å­¦ä¹ ç‡æˆ–å¢åŠ æ¢¯åº¦è£å‰ª
- Lossä¸ä¸‹é™ï¼šæ£€æŸ¥æ•°æ®åŠ è½½ï¼ŒéªŒè¯åˆ†è¯
- OOMé”™è¯¯ï¼šå‡å°‘`--batch_size`æˆ–`--context_len`

## ä½œä¸šè¯´æ˜

è¯¦ç»†çš„ä½œä¸šè¦æ±‚å’Œç†è®ºèƒŒæ™¯ï¼Œè¯·å‚é˜…ï¼š
- [cs336_spring2025_assignment1_basics.pdf](./cs336_spring2025_assignment1_basics.pdf)

## è®¸å¯è¯

æœ¬ä»£ç ä½œä¸ºStanford CS336çš„ä¸€éƒ¨åˆ†ï¼Œä»…ä¾›æ•™è‚²ç›®çš„ä½¿ç”¨ã€‚
